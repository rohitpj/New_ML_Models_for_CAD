{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 262,
     "status": "error",
     "timestamp": 1713201359035,
     "user": {
      "displayName": "Rohit John",
      "userId": "06224148796144481458"
     },
     "user_tz": -60
    },
    "id": "AggX2-E8eVxk",
    "outputId": "da6e7e7c-c781-4149-abb0-4751286a23a3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Data, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tOLUQBbIra5E"
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHy2jTfrtQVM"
   },
   "outputs": [],
   "source": [
    "def adjacency_to_edge_index(adjacency_matrix):\n",
    "    edge_index = []\n",
    "    for i in range(adjacency_matrix.shape[0]):\n",
    "        for j in range(adjacency_matrix.shape[1]):\n",
    "            if adjacency_matrix[i, j] != 0:\n",
    "                edge_index.append([i, j])\n",
    "    return torch.tensor(edge_index, dtype=torch.long).t()\n",
    "\n",
    "\n",
    "def load_graph_and_label(graph_file, label_file):\n",
    "    features_df = pd.read_csv(graph_file, delimiter=',', header=None, skiprows=1, nrows=3)\n",
    "    features_df = features_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    if features_df.isnull().values.any():\n",
    "        features_df = features_df.fillna(0)\n",
    "    feature_tensor = torch.tensor(features_df.values, dtype=torch.float)\n",
    "    adjacency_matrix = pd.read_csv(graph_file, delimiter=',', header=None, skiprows=5)\n",
    "    adjacency_matrix_tensor = torch.tensor(adjacency_matrix.values, dtype=torch.long)\n",
    "\n",
    "    edge_index = adjacency_to_edge_index(adjacency_matrix_tensor)\n",
    "\n",
    "\n",
    "    #print(edge_index.shape)\n",
    "    label = pd.read_csv(label_file, header=None).values[0,0]\n",
    "\n",
    "    data = Data(x=feature_tensor, edge_index=edge_index, y=torch.tensor(label, dtype=torch.long))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8cHDRng3uvJ"
   },
   "outputs": [],
   "source": [
    "label_mappings = {\n",
    "    0: [0, 1, 2],\n",
    "    1: [0, 2, 1],\n",
    "    2: [1, 0, 2],\n",
    "    3: [1, 2, 0],\n",
    "    4: [2, 0, 1],\n",
    "    5: [2, 1, 0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKil48gDHH0S"
   },
   "outputs": [],
   "source": [
    "gnn_graphs_dir = '/content/drive/MyDrive/GNN_Colab/sampled_gnn_data-Copy/gnn_graphs'\n",
    "gnn_labels_dir = \"/content/drive/MyDrive/GNN_Colab/sampled_gnn_data-Copy/gnn_labels\"\n",
    "\n",
    "\"\"\"\n",
    "dataset = []\n",
    "i=0\n",
    "# Iterate through the files in the graphs directory\n",
    "for filename in os.listdir(gnn_graphs_dir):\n",
    "      if filename.endswith(\".csv\"):\n",
    "          graph_file = os.path.join(gnn_graphs_dir, filename)\n",
    "          label_file = os.path.join(gnn_labels_dir, filename.replace('_combined.csv', '_label.txt'))\n",
    "          #/content/drive/MyDrive/GNN_Colab/gnn_data/gnn_graphs\n",
    "          # Check if both the graph and label files exist\n",
    "          if os.path.exists(graph_file) and os.path.exists(label_file):\n",
    "              data = load_graph_and_label(graph_file, label_file)\n",
    "              dataset.append(data)\n",
    "          else:\n",
    "              print(f\"Files not found for: {filename} \",graph_file,label_file)\"\"\"\n",
    "\n",
    "dataset_file_path = '/content/drive/MyDrive/GNN_Colab/final_full_dataset_file.pt'  # Specify the path to save your dataset\n",
    "torch.save(dataset, dataset_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Klack7uqTodM"
   },
   "outputs": [],
   "source": [
    "dataset = torch.load('/content/drive/MyDrive/GNN_Colab/final_full_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8L6qe1O0cMYi"
   },
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmzfvbyErBCB"
   },
   "outputs": [],
   "source": [
    "random.shuffle(dataset)\n",
    "dataset_groups = defaultdict(list)\n",
    "\n",
    "for data in dataset:\n",
    "    if data.edge_index.dim() == 2:\n",
    "        num_edges = data.edge_index.size(1)\n",
    "    else:\n",
    "        num_edges = 0\n",
    "\n",
    "    dataset_groups[num_edges].append(data)\n",
    "\n",
    "for num_edges in dataset_groups:\n",
    "    random.shuffle(dataset_groups[num_edges])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Loz5kzLKX48X"
   },
   "outputs": [],
   "source": [
    "def dataset_to_dataframe(dataset):\n",
    "    data_list = []\n",
    "\n",
    "    for data in dataset:\n",
    "        x_flattened = data.x.reshape(-1).numpy()\n",
    "        data_dict = {'features': x_flattened}\n",
    "        if hasattr(data, 'y'):\n",
    "            data_dict['label'] = data.y.item()\n",
    "        if hasattr(data, 'name'):\n",
    "            data_dict['name'] = data.name\n",
    "        data_list.append(data_dict)\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "df = dataset_to_dataframe(dataset)\n",
    "\n",
    "features_df = pd.DataFrame(df['features'].tolist(), index=df.index)\n",
    "df_expanded = pd.concat([df.drop(columns=['features']), features_df], axis=1)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "output_file_path = '/content/drive/MyDrive/CAD_Project/extra_features_data'\n",
    "df_expanded.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYMeNPbrY81L"
   },
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    if hasattr(data, 'name'):\n",
    "        del data.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAkzvN0BqrRO"
   },
   "outputs": [],
   "source": [
    "halved_dataset= dataset[:int(len(dataset) * 0.25)]\n",
    "train_dataset = halved_dataset[:int(len(halved_dataset) * 0.8)]\n",
    "test_dataset = halved_dataset[int(len(halved_dataset) * 0.2):]\n",
    "acc_test_dataset = dataset#[int(len(dataset) * 0.2):]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "acc_test_loader = DataLoader(acc_test_dataset, batch_size=32, shuffle=False)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZ8OeoP8Uxkt"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "dataset_modified = copy.deepcopy(dataset)\n",
    "\n",
    "for data in dataset_modified:\n",
    "    # Skips 5th and 6th elements\n",
    "    data.x = torch.cat((data.x[:, :4], data.x[:, 6:-2]), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1713132144284,
     "user": {
      "displayName": "Rohit John",
      "userId": "06224148796144481458"
     },
     "user_tz": -60
    },
    "id": "wnqkFvyVtvF8",
    "outputId": "bdb4c34c-dbcd-4dad-ae42-de3a1c387cd3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch_geometric.nn as pyg_nn\\n\\nclass VariableOrderGNN(nn.Module):\\n    def __init__(self, input_dim, hidden_dim):\\n        super(VariableOrderGNN, self).__init__()\\n\\n        # GCN layers\\n        self.conv1 = pyg_nn.GCNConv(input_dim, hidden_dim)\\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\\n        self.conv2 = pyg_nn.GCNConv(hidden_dim, hidden_dim * 2)\\n        self.bn2 = nn.BatchNorm1d(hidden_dim * 2)\\n        self.conv3 = pyg_nn.GCNConv(hidden_dim * 2, hidden_dim * 4)\\n        self.bn3 = nn.BatchNorm1d(hidden_dim * 4)\\n\\n        # Dropout layer for regularization\\n        self.dropout = nn.Dropout(0.25)\\n\\n        # Fully connected layers\\n        self.fc1 = nn.Linear(hidden_dim * 4, hidden_dim * 4)\\n        self.fc2 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\\n        self.fc3 = nn.Linear(hidden_dim * 2, hidden_dim)  # Additional layer\\n        self.fc4 = nn.Linear(hidden_dim, hidden_dim // 2)  # Additional layer\\n        self.fc_final = nn.Linear(hidden_dim // 2, 6)  # Output dim = 6 for 6 orderings\\n\\n    def forward(self, data):\\n        x, edge_index, batch = data.x, data.edge_index, data.batch\\n\\n        # Apply GCN layers with ReLU activation and batch normalization\\n        x = F.relu(self.bn1(self.conv1(x, edge_index)))\\n        x = self.dropout(x)\\n        x = F.relu(self.bn2(self.conv2(x, edge_index)))\\n        x = self.dropout(x)\\n        x = F.relu(self.bn3(self.conv3(x, edge_index)))\\n\\n        # Global mean pooling to aggregate node features into a graph-level feature\\n        x = pyg_nn.global_mean_pool(x, batch)\\n\\n        # Passing through fully connected layers\\n        x = F.relu(self.fc1(x))\\n        x = self.dropout(x)\\n        x = F.relu(self.fc2(x))\\n        x = self.dropout(x)\\n        x = F.relu(self.fc3(x))\\n        x = self.dropout(x)\\n        x = F.relu(self.fc4(x))\\n        x = self.dropout(x)\\n        x = self.fc_final(x)\\n\\n        return x\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VariableOrderGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(VariableOrderGNN, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = pyg_nn.GCNConv(hidden_dim, hidden_dim * 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.conv3 = pyg_nn.GCNConv(hidden_dim * 2, hidden_dim * 4)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim * 4)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, 6)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        #data.edge_index = data.edge_index.t()\n",
    "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x, edge_index)))\n",
    "\n",
    "        x = pyg_nn.global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103005,
     "status": "ok",
     "timestamp": 1713132250560,
     "user": {
      "displayName": "Rohit John",
      "userId": "06224148796144481458"
     },
     "user_tz": -60
    },
    "id": "rWAEiym2ubdT",
    "outputId": "3cd7130a-c9d0-40ee-b73f-c9f2d2c94042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 1.7838718730050165\n",
      "Epoch: 20, Loss: 1.7828442765018655\n",
      "Epoch: 30, Loss: 1.7814240736390634\n",
      "Epoch: 40, Loss: 1.780410682372605\n"
     ]
    }
   ],
   "source": [
    "model = VariableOrderGNN(input_dim=14, hidden_dim=5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #print(\"Model output shape:\", output.shape)\n",
    "\n",
    "        loss = loss_fn(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "for epoch in range(1, 50):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1713131451460,
     "user": {
      "displayName": "Rohit John",
      "userId": "06224148796144481458"
     },
     "user_tz": -60
    },
    "id": "qVxE3dEaiOp8",
    "outputId": "619fc9ac-78f6-4a8f-f72f-b17c097cccb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.2320\n"
     ]
    }
   ],
   "source": [
    " def test_accuracy(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, dim=1)\n",
    "            total += data.y.size(0)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Test the model\n",
    "test_acc = test_accuracy(acc_test_loader)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
